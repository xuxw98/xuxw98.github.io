<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="EmbodiedSAM: Online Segment Any 3D Thing in Real Time">
    <meta name="author" content="Hang Yin,
                                 Xiuwei Xu,
                                 Zhenyu Wu,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</h2>
    <h3>NeurIPS 2024</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://bagh2178.github.io/">Hang Yin</a><sup>1*</sup>&nbsp;&nbsp;
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1*†</sup>&nbsp;&nbsp;
        <a href="https://gary3410.github.io/">Zhenyu Wu</a><sup>2</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1‡</sup>
        </span>
        <br><br>
        <sup>1</sup>Tsinghua University&nbsp;&nbsp;<sup>2</sup>Beijing University of Posts and Telecommunications<br>
        <br><br>
        <a href="https://arxiv.org/abs/xxxx" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/bagh2178/SG-Nav" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <!-- <a href="https://zhuanlan.zhihu.com/p/704435537" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/zhihu.png" alt="code" style="vertical-align: middle;">
            &nbsp;中文解读 (Zhihu)
        </a> -->
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">

        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/li1ru0iWvxU" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            If video does not load, click <a href="https://cloud.tsinghua.edu.cn/f/f5f236c0ed3c4951af13/?dl=1">HERE</a> to download.
        </p>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            In this paper, we propose a new framework for zero-shot object navigation. Existing zero-shot object navigation methods prompt LLM with the text of spatially closed objects, which lacks enough scene context for in-depth reasoning. To better preserve the information of environment and fully exploit the reasoning ability of LLM, we propose to represent the observed scene with 3D scene graph. The scene graph encodes the relationships between objects, groups and rooms with a LLM-friendly structure, for which we design a hierarchical chain-of-thought prompt to help LLM reason the goal location according to scene context by traversing the nodes and edges. Moreover, benefit from the scene graph representation, we further design a re-perception mechanism to empower the object navigation framework with the ability to correct perception error. We conduct extensive experiments on MP3D, HM3D and RoboTHOR environments, where SG-Nav surpasses previous state-of-the-art zero-shot methods by more than \textbf{10\%} SR on all benchmarks, while the decision process is explainable. To the best of our knowledge, SG-Nav is the first zero-shot method that achieves even higher performance than supervised object navigation methods on the challenging MP3D benchmark.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>Overall framework of our approach.</b> We construct a hierarchical 3D scene graph as well as an occupancy map online. At each step, we divide the scene graph into several subgraphs, each of which is prompted to LLM with a hierarchical chain-of-thought for structural understanding of the scene context. We interpolate the probability score of each subgraph to the frontiers and select the frontier with highest score for exploration. This decision is also explainable by summarizing the reasoning process of the LLM. With the scene graph representation, we further design a re-perception mechanism, which helps the agent give up false positive goal object by continuous credibility judgement.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" alt="pipeline" width="90%">
            </div>
        </div>
    </div>


    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We evaluate our method on MP3D, HM3D and RoboTHOR.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp1.png" alt="pipeline" width="85%">
            </div>
        </div>
        <p>
            <b>Object-goal navigation results</b> on MP3D, HM3D and RoboTHOR. We compare the Success Rate (SR) and success rate weighted by
path length (SPL) of state-of-the-art methods in different settings.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp2.png" alt="pipeline" width="70%">
            </div>
        </div>
        <p>
            <b>Left:</b> Per category Success Rate on MP3D. <b>Right:</b> Time cost of connecting n edges for online 3D scene graph construction. 
        </p>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
                @article{yin2024sgnav, 
                      title={SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation}, 
                      author={Hang Yin and Xiuwei Xu and Zhenyu Wu and Jie Zhou and Jiwen Lu},
                      journal={arXiv preprint arXiv},
                      year={2024}
                }
            </div>
        </div>
    </div>

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=em2C5MHU7JV42B9o0S3nC3jnRAl5WRDYf-3CBK5CubI&cl=ffffff&w=a"></script>    
        </div>        
        <br>
        &copy; Hang Yin | Last update: Oct. 8, 2024
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>

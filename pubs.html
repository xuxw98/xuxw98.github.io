<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiuwei Xu</title>
  
  <meta name="author" content="Xiuwei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <style>
    .navbar ul {
        list-style-type: none;
        margin: 0;
        padding: 0;
        overflow: hidden;
        border: 1px solid #e7e7e7;
        background-color: #f3f3f3;
    }

    .navbar li {
        float: left;
    }

    .navbar li a {
        display: block;
        color: #666;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
    }

    .navbar li a:hover:not(.active) {
        background-color: #ddd;
    }

    .navbar li a.active {
        color: white;
        background-color: #4CAF50;
    }
  </style>
</head>

<div class="navbar">
    <ul>
      <li><a href="index.html">Main Page</a></li>
      <li><a href="pubs.html">Publications</a></li>
      <li><a href="projects.html">Projects</a></li>
      <li style="float:right"><a class="active" href="#about">About</a></li>
    </ul>
</div>

<body>
  <table style="width:100%;max-width:860px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiuwei Xu | 许修为</name>
              </p>
              <p> 
                I am a fourth year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2021, I obtained my B.Eng. in the Department of Automation, Tsinghua University.
              </p>
              <p>
              I work on computer vision and robotics. My current research focuses on:
                <li style="margin: 5px;" >
                <b style="color:brown">General manipulation</b> that learns generalized manipulation skills with data-efficient imitation learning and 3D policy.
              </li>
              <li style="margin: 5px;" >
                <b style="color:green">Visual navigation</b> that enables robots to explore the environment according to multimodal instructions.
              </li>
              <li style="margin: 5px;" >
                <b style="color:orange">3D scene perception</b> that accurately and efficiently understands the dynamic 3D scenes captured by robotic agent.
              </li>
               <!-- <li style="margin: 5px;" >
                <b style="color:orange">3D reconstruction</b> that builds 3D scenes from raw sensor inputs in online and real-time, especially SLAM and 3D gaussians.
              </li> -->
              </p>
              <p style="text-align:center">
                <a href="mailto:xxw21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://xuxw98.github.io/files/XXW_CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4G627acAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xuxw98"> Github </a>
              </p>
            </td>
            <td style="padding:0%;width:100%;max-width:100%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/xxw.jpg">
            </td>
          </tr>
        </tbody></table>

        <!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p>
            *Equal contribution, &nbsp;&nbsp; <sup>†</sup>Project leader.
          </p>
          </td>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p>*Equal contribution, &nbsp;&nbsp; <sup>†</sup>Project leader.</p>
              <p><heading>Preprint</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/R2RGen.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>R2RGen: Real-to-Real 3D Data Generation for Spatially Generalized Manipulation</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>*, <a href="https://github.com/ponymaay">Angyuan Ma</a>*, Hankun Li, Bingyao Yu, <a href="http://www.zhengzhu.net/">Zheng Zhu</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>arXiv</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2510.08547">[arXiv]</a> <a href="https://r2rgen.github.io/">[Project Page]</a> <a href="https://colab.research.google.com/drive/19zrG9IG0DkbbJzB12gKC1Ur2r--jZzRC?usp=sharing">[Colab]</a>
              <br>
              <p> We propose a real-to-real 3D data generation framework for robotic manipulation. R2RGen generates spatially diverse manipulation demonstrations for training real-world policies, requiring only one human demonstration without simulator setup.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TAPA.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Task Planning with Large Language Models</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2307.01848">[arXiv]</a> <a href="https://github.com/Gary3410/TaPA">[Code]</a> <a href="https://gary3410.github.io/TaPA/">[Project Page]</a> <a href="https://huggingface.co/spaces/xuxw98/TAPA">[Demo]</a>
              <br>
              <p> We propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MoTo.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General Mobile Manipulation</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>*, <a href="https://github.com/ponymaay">Angyuan Ma</a>*, <strong>Xiuwei Xu</strong><sup>†</sup>, <a href="https://bagh2178.github.io/">Hang Yin</a>, <a href="https://github.com/liangyn22">Yinan Liang</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2509.01658">[arXiv]</a> <a href="https://gary3410.github.io/MoTo/">[Project Page]</a>
              <br>
              <p>We propose a general framework for mobile manipulation, which can be divided into docking point selection and fixed-base manipulation. We model the docking point selection stage as an optimization process, to let the agent move and touch target keypoint under several constraints.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/GC-VLN.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>GC-VLN: Instruction as Graph Constraints for Training-free Vision-and-Language Navigation</papertitle>
              <br>
              <a href="https://bagh2178.github.io/">Hang Yin</a>*, Haoyu Wei*, <strong>Xiuwei Xu</strong><sup>†</sup>, <a href="https://gwxuan.github.io/">Wenxuan Guo</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2509.10454">[arXiv]</a> <a href="https://github.com/bagh2178/GC-VLN">[Code]</a> <a href="https://bagh2178.github.io/GC-VLN/">[Project Page]</a>
              <br>
              <p>We propose a unified 3D graph representation for zero-shot vision-and-language navigation. By modeling instruction graph as constraints, we can solve the optimal navigation path accordingly. Wrong exploration can also be handled by graph-based backtracking.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/IGL-Nav.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation</papertitle>
              <br>
              <a href="https://gwxuan.github.io/">Wenxuan Guo</a>*, <strong>Xiuwei Xu</strong>*, <a href="https://bagh2178.github.io/">Hang Yin</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://ivg.au.tsinghua.edu.cn/~jfeng/index.html">Jianjiang Feng</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2508.00823">[arXiv]</a> <a href="https://github.com/GWxuan/IGL-Nav">[Code]</a> <a href="https://gwxuan.github.io/IGL-Nav/">[Project Page]</a>
              <br>
              <p>We propose IGL-Nav, an incremental 3D Gaussian localization framework for image-goal navigation. It supports challenging scenarios where the camera for goal capturing and the agent's camera have very different intrinsics and poses, e.g., a cellphone and a RGB-D camera.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DepthGS.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Pseudo Depth Meets Gaussian: A Feed-forward RGB SLAM Baseline</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>*, <strong>Xiuwei Xu</strong>*, <a href="https://github.com/wangyr22">Yirui Wang</a>, Hao Wang, <a href="https://wzzheng.net/">Wenzhao Zheng</a>, <a href="https://andytang15.github.io/">Yansong Tang</a>, Haibin Yan, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2508.04597">[arXiv]</a> <a href="https://github.com/wangyr22/DepthGS">[Code]</a> 
              <br>
              <p>We propose an online RGB SLAM method that utilizes only monocular RGB input, eliminating the need for depth sensors or expensive iterative pose optimization. We propose to leverage 3DGS-based optimization to mitigate the error of depth estimator.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/EIF-Unknown.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Instruction Following in Unknown Environments</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2406.11818">[arXiv]</a> <a href="https://github.com/Gary3410/eif_unknown">[Code]</a> <a href="https://gary3410.github.io/eif_unknown/">[Project Page]</a>
              <br>
              <p>Our embodied agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. which can complete complex human instructions such as making breakfast, tidying bedrooms and cleaning bathrooms in house-level scenes.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TSP3D.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Text-guided Sparse Voxel Pruning for Efficient 3D Visual Grounding</papertitle>
              <br>
              <a href="https://gwxuan.github.io/">Wenxuan Guo</a>*, <strong>Xiuwei Xu</strong>*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://ivg.au.tsinghua.edu.cn/~jfeng/index.html">Jianjiang Feng</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025 (<em><strong style="color:red;">Highlight, All Strong Accept</strong></em>)
              <br>
              <a href="https://arxiv.org/abs/2502.10392">[arXiv]</a> <a href="https://github.com/GWxuan/TSP3D">[Code]</a> <a href="https://zhuanlan.zhihu.com/p/29557016028">[中文解读]</a>
              <br>
              <p>We propose TSP3D, an efficient multi-level convolution architecture for 3D visual grounding. TSP3D achieves superior performance compared to previous approaches in both accuracy and inference speed.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/UniGoal.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>UniGoal: Towards Universal Zero-shot Goal-oriented Navigation</papertitle>
              <br>
              <a href="https://bagh2178.github.io/">Hang Yin</a>*, <strong>Xiuwei Xu</strong>*<sup>†</sup>, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2503.10630">[arXiv]</a> <a href="https://github.com/bagh2178/UniGoal">[Code]</a> <a href="https://bagh2178.github.io/UniGoal/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/30973430092">[中文解读]</a>
              <br>
              <p>We propose UniGoal, a unified graph representation for zero-shot goal-oriented navigation. Based on online 3D scene graph prompting for LLM, our method can be directly applied to different kinds of scenes and goals without training.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MoManipVLA.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MoManipVLA: Transferring Vision-language-action Models for General Mobile Manipulation</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, Yuheng Zhou, <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2025
              <br>
              <a href="https://arxiv.org/abs/2503.13446">[arXiv]</a> <a href="https://gary3410.github.io/momanipVLA/">[Project Page]</a>
              <br>
              <p>We propose an efficient policy adaptation framework named MoManipVLA to transfer pre-trained VLA models of fixed-base manipulation to mobile manipulation. We utilize pre-trained VLA models to generate waypoints of the end-effector with high generalization ability and devise motion planning objectives to maximize the physical feasibility of generated trajectory.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ESAM.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://github.com/XXXCARREY">Huangxing Chen</a>, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 (<em><strong style="color:red;">Oral, Top 1.8% Submission</strong></em>)
              <br>
              <a href="https://arxiv.org/abs/2408.11811">[arXiv]</a> <a href="https://github.com/xuxw98/ESAM">[Code]</a> <a href="https://xuxw98.github.io/ESAM/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/23105869992">[中文解读]</a>
              <br>
              <p>We presented ESAM, an efficient framework that leverages vision foundation models for online, real-time, fine-grained, generalized and open-vocabulary 3D instance segmentation.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SG-Nav.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</papertitle>
              <br>
              <a href="https://bagh2178.github.io/">Hang Yin</a>*, <strong>Xiuwei Xu</strong>*<sup>†</sup>, <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.08189">[arXiv]</a> <a href="https://github.com/bagh2178/SG-Nav">[Code]</a> <a href="https://bagh2178.github.io/SG-Nav/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/909651478">[中文解读]</a>
              <br>
              <p> We propose a training-free object-goal navigation framework by leveraging LLM and VFMs. We construct an online hierarchical 3D scene graph and prompt LLM to exploit structure information contained in subgraphs for zero-shot decision making.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Q-VLM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Q-VLM: Post-training Quantization for Large Vision-Language Models</papertitle>
              <br>
              <a href="https://changyuanwang17.github.io/">Changyuan Wang</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://andytang15.github.io/">Yansong Tang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.08119">[arXiv]</a> <a href="https://github.com/ChangyuanWang17/QVLM">[Code]</a>
              <br>
              <p> We propose a post-training quantization framework of large vision-language models (LVLMs). Our method compresses the memory by 2.78x and increase the generate speed by 1.44x on 13B LLaVA model without performance degradation on diverse multi-modal reasoning tasks.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DSP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>3D Small Object Detection with Dynamic Spatial Pruning</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>*, Zhihao Sun*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Hongmin Liu, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2305.03716">[arXiv]</a> <a href="https://github.com/xuxw98/DSPDet3D">[Code]</a> <a href="https://xuxw98.github.io/DSPDet3D/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/714402773">[中文解读]</a>
              <br>
              <p> We propose an effective and efficient 3D detector named DSPDet3D for detecting small objects. By scaling up the spatial resolution of feature maps and pruning uninformative scene representaions, DSPDet3D is able to capture detailed local geometric information while keeping low memory footprint and latency.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Online3D.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Memory-based Adapters for Online 3D Scene Perception</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>*, Chong Xia*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.06974">[arXiv]</a> <a href="https://github.com/xuxw98/Online3D">[Code]</a> <a href="https://xuxw98.github.io/Online3D/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/704435537">[中文解读]</a>
              <br>
              <p> We propose a model and task-agnostic plug-and-play module, which converts offline 3D scene perception models (receive reconstructed point clouds) to online perception models (receive streaming RGB-D videos). </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ADPDM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Towards Accurate Data-free Quantization for Diffusion Models</papertitle>
              <br>
              <a href="https://changyuanwang17.github.io/">Changyuan Wang</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://andytang15.github.io/">Yansong Tang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 (<em><strong style="color:red;">Highlight, Top 2.8% Submission</strong></em>)
              <br>
              <a href="https://arxiv.org/abs/2305.18723">[arXiv]</a> <a href="https://github.com/ChangyuanWang17/APQ-DM">[Code]</a>
              <br>
              <p> We propose a post-training quantization framework to compress diffusion models, which performs group-wise quantization to minimize rounding errors across time steps and selects generated contents in the optimal time steps for calibration. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR++.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Learning Data-Efficient 3D Object Detector with Shape Guidance</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 23.6)</em>, 2023
              <br>
              <a href="https://xuxw98.github.io/files/BR++.pdf">[PDF]</a> <a href="https://xuxw98.github.io/files/BR++-supp.pdf">[Supp]</a> <a href="https://github.com/xuxw98/BackToReality">[Code]</a> 
              <br>
              <p> We extend BR to BR++ by introducing differentiable label enhancement and label-assisted self-training. Our approach surpasses current weakly-supervised and semi-supervised methods by a large margin, and achieves comparable detection performance with some fully-supervised methods with less than 5% of the labeling labor. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MCUFormer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</papertitle>
              <br>
              <a href="https://github.com/liangyn22">Yinan Liang</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://andytang15.github.io/">Yansong Tang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2310.16898">[arXiv]</a> <a href="https://github.com/liangyn22/MCUFormer">[Code]</a> <a href="https://mp.weixin.qq.com/s/j4-C2eDSpJPCAPp1XYwEZA">[中文解读]</a>
              <br>
              <p> We propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. </p>
            </td>
          </tr>

        </tbody></table>

        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BSC.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.15493">[arXiv]</a> <a href="https://xuxw98.github.io/files/BSC_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a binary sparse convolutional network called BSC-Net for efficient point cloud analysis. With the presented shifted sparse convolution operation and efficient search method, we reduce the quantization error for sparse convolution without additional computation overhead. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Quantformer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantformer: Learning Extremely Low-precision Vision Transformers</papertitle>
              <br>
              <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://changyuanwang17.github.io/">Changyuan Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 23.6)</em>, 2022
              <br>
              <a href="https://xuxw98.github.io/files/Quantformer.pdf">[PDF]</a> <a href="https://xuxw98.github.io/files/Quantformer-supp.pdf">[Supp]</a> <a href="https://github.com/ZiweiWangTHU/Quantformer">[Code]</a> 
              <br>
              <p> We propose the extremely low-precision vision transformers in 2-4 bits, where a self-attention rank consistency loss and a group-wise quantization strategy are presented for quantization error minimization. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Yifan Wang, <a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=en">Yu Zheng</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.05238">[arXiv]</a> <a href="https://github.com/wyf-ACCEPT/BackToReality">[Code]</a> <a href="https://xuxw98.github.io/files/BR_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with only annotations of object centers. We convert the weak annotations into virtual scenes with synthetic 3D shapes and apply domain adaptation to train a size-aware detector for real scenes. </p>
            </td>
          </tr>

        </tbody></table>


        
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
    <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=26-EpITiANieKUbBkP3kYqH25_ayJEKe2zYZjXiBHyE"></script>
    </div>        
    <br>
      &copy; Xiuwei Xu | Last updated: March 12, 2025
</center></p>
</body>

</html>

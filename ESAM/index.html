<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description"
          content="EmbodiedSAM: Online Segment Any 3D Thing in Real Time">
    <meta name="author" content="Xiuwei Xu,
                                 Huangxing Chen,
                                 Linqing Zhao,
                                 Ziwei Wang,
                                 Jie Zhou,
                                 Jiwen Lu">

    <title>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</title>
    <!-- Bootstrap core CSS -->
    <!--link href="bootstrap.min.css" rel="stylesheet"-->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css"
          integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <!-- Custom styles for this template -->
    <link href="offcanvas.css" rel="stylesheet">
    <!--    <link rel="icon" href="img/favicon.gif" type="image/gif">-->
</head>

<body>
<div class="jumbotron jumbotron-fluid">
    <div class="container"></div>
    <h2>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</h2>
    <h3>arXiv 2024</h3>
<!--            <p class="abstract">An interpretable, data-efficient, and scalable neural scene representation.</p>-->
    <hr>
    <p>
        <span style="white-space: nowrap; font-size:larger">
        <a href="https://xuxw98.github.io/">Xiuwei Xu</a><sup>1</sup>&nbsp;&nbsp;
        Huangxing Chen<sup>1</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a><sup>1</sup>&nbsp;&nbsp;
        <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a><sup>2</sup>&nbsp;&nbsp;
        <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a><sup>1</sup>&nbsp;&nbsp;
        <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a><sup>1&#8224;</sup>
        </span>
        <br><br>
        <sup>1</sup>Tsinghua University&nbsp;&nbsp;<sup>2</sup>Nanyang Technological University<br>
        <br><br>
        <a href="https://arxiv.org/abs/xxxx" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/file.png" alt="paper" style="vertical-align: middle;">
            &nbsp;Paper (arXiv)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <a href="https://github.com/xuxw98/ESAM" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/github.png" alt="code" style="vertical-align: middle;">
            &nbsp;Code (GitHub)
        </a>&nbsp;&nbsp;&nbsp;&nbsp;
        <!-- <a href="https://zhuanlan.zhihu.com/p/704435537" target="_blank" style="color: #1E90FF;">
            <img src="https://img.icons8.com/material-outlined/24/000000/zhihu.png" alt="code" style="vertical-align: middle;">
            &nbsp;中文解读 (Zhihu)
        </a> -->
    </p>

    <!-- <div class="btn-group" role="group" aria-label="Top menu">
        <a class="btn btn-primary" href="https://arxiv.org/abs/2006.09661">Paper</a>
        <a class="btn btn-primary" href="https://colab.research.google.com/github/vsitzmann/siren/blob/master/explore_siren.ipynb">Colab Notebook</a>
        <a class="btn btn-primary" href="https://dcato98.github.io/playground/#activation=sine">Tensorflow Playground</a>
        <a class="btn btn-primary" href="https://github.com/vsitzmann/siren">Code</a>
        <a class="btn btn-primary" href="https://drive.google.com/drive/u/1/folders/1_iq__37-hw7FJOEUK1tX7mdp8SKB368K">Data</a>
    </div> -->
</div>

<div class="container">

        <div class="vcontainer">
        <iframe class='video' src="https://www.youtube.com/embed/LI1y9ugUPdk" frameborder="0"
                allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen></iframe>
        </div>
        <p>
            If video does not load, click <a href="https://cloud.tsinghua.edu.cn/f/f75279f89bf64720b8ec/?dl=1">HERE</a> to download.
        </p>

    <div class="section">
        <h2>Abstract</h2>
        <hr>
        <p>
            Embodied tasks require the agent to fully understand 3D scenes simultaneously with its exploration, so an <b>online</b>, <b>real-time</b>, <b>ﬁne-grained</b> and <b>highly-generalized</b> 3D perception model is desperately needed. Since high-quality 3D data is limited, directly training such a model in 3D is almost infeasible. Meanwhile, vision foundation models (VFM) has revolutionized the ﬁeld of 2D computer vision with superior performance, which makes the use of VFM to assist embodied 3D perception a promising direction. However, most existing VFM-assisted 3D perception methods are either ofﬂine or too slow that cannot be applied in practical embodied tasks. <b>In this paper, we aim to leverage Segment Anything Model (SAM) for real-time 3D instance segmentation in an online setting.</b> This is a challenging problem since future frames are not available in the input streaming RGB-D video, and an instance may be observed in several frames so object matching between frames is required. To address these challenges, we ﬁrst propose a geometric-aware query lifting module to represent the 2D masks generated by SAM by 3D-aware queries, which is then iteratively reﬁned by a dual-level query decoder. In this way, the 2D masks are transferred to ﬁne-grained shapes on 3D point clouds. Beneﬁt from the query representation for 3D masks, we can compute the similarity matrix between the 3D masks from different views by efﬁcient matrix operation, which enables real-time inference. Experiments on ScanNet, ScanNet200, SceneNN and 3RScan show our method achieves leading performance even compared with ofﬂine methods. Our method also demonstrates great generalization ability in several zero-shot dataset transferring experiments and show great potential in open-vocabulary and data-efﬁcient setting. Code is available, with only one RTX 3090 GPU required for training and evaluation.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/teaser.png" alt="pipeline" width="100%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Approach</h2>
        <hr>
        <p>
            <b>Overall framework of our approach.</b> At a new time instant, we first adopt SAM to generate 2D instance masks. We propose a geometric-aware query lifting module to lift 2D masks to 3D queries while preserving fine-grained shape information. Queries are refined by a dual-level decoder, which enables efficient cross-attention and generates fine-grained point-wise masks. Then current 3D mask is merged into previous masks by a fast query merging strategy.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/pipeline.png" alt="pipeline" width="90%">
            </div>
        </div>
    </div>

    <div class="section">
        <h2>Visualization</h2>
        <hr>
        <p>
            We visualize ESAM on both offline and online setting.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/vis1.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            Visualization results of different 3D instance segmentation methods on ScanNet200 dataset. As highlighted in red boxes, SAM3D predicts noisy masks while SAI3D tends to over segment an instance into multiple parts.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/vis2.png" alt="pipeline" width="90%">
            </div>
        </div>
        <p>
            Online visualization of ESAM on ScanNet200 dataset. Refer to the video demo on this page for more details.
        </p>
    </div>

    <div class="section">
        <h2>Experiments</h2>
        <hr>
        <p>
            We evaluate our method on ScanNet, ScanNet200, SceneNN and 3RScan.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp1.png" alt="pipeline" width="85%">
            </div>
        </div>
        <p>
            <b>Class-agnostic 3D instance segmentation results of different methods on ScanNet200 dataset.</b> The unit of Speed is ms per frame, where the speed of VFM and other parts are reported separately.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp2.png" alt="pipeline" width="70%">
            </div>
        </div>
        <p>
            <b>Dataset transfer results of different methods from ScanNet200 to SceneNN and 3RScan.</b> We directly evaluate the models in Table 1 on other datasets to show their generalization ability.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp3.png" alt="pipeline" width="75%">
            </div>
        </div>
        <p>
            <b>3D instance segmentation results of different methods on ScanNet and SceneNN datasets.</b> ESAM achieves leading accuracy and speed compared with previous state-of-the-art online 3D perception methods. 
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp4.png" alt="pipeline" width="33%">
            </div>
        </div>
        <p>
            <b>Open-vocabulary 3D instance segmentation results on ScanNet200 dataset.</b> As an online method, ESAM surpasses the offline SAI3D by a large margin, which demonstrates great potential for practical application.
        </p>
        <!-- add a image here-->
        <div class="row align-items-center">
            <div class="col justify-content-center text-center">
                <img src="img/exp5.png" alt="pipeline" width="35%">
            </div>
        </div>
        <p>
            <b>Performance of ESAM when trained with partial training set.</b> Even with only 20% trainign data, ESAM still achieves state-of-the-art performance compared with the baselines in Table 1.
        </p>
    </div>

    <div class="section">
        <h2>Bibtex</h2>
        <hr>
            <div class="bibtexsection">
        @article{xu2024esam, 
          title={EmbodiedSAM: Online Segment Any 3D Thing in Real Time}, 
          author={Xiuwei Xu and Huangxing Chen and Linqing Zhao and Ziwei Wang and Jie Zhou and Jiwen Lu},
          journal={arXiv preprint arXiv:xxxx},
          year={2024}
        }
            </div>
        </div>
    </div>

    <hr>
</div>

    <p><center>
        <div id="clustrmaps-widget" style="width:30%">
            <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=P_pGhtV2G_G_lhyYDPq0rP9ps_zT8ZXq40Zk9Cjqtuc&cl=ffffff&w=a"></script>    
        </div>        
        <br>
        &copy; Xiuwei Xu | Last update: March. 4, 2024
    </center></p>

<script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js"
        integrity="sha384-Q6E9RHvbIyZFJoft+2mJbHaEWldlvI9IOYy5n3zV9zzTtmI3UksdQRVvoxMfooAo"
        crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/js/bootstrap.min.js"
        integrity="sha384-OgVRvuATP1z7JjHLkuOU7Xw704+h835Lr+6QL9UvYjZE3Ipu6Tp75j7Bh/kR0JKI"
        crossorigin="anonymous"></script>

</body>
</html>

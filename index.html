<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiuwei Xu</title>
  
  <meta name="author" content="Xiuwei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiuwei Xu | 许修为</name>
              </p>
              <p> 
                I am a second year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2021, I obtained my B.Eng. in the Department of Automation, Tsinghua University.
              </p>
              <p>
              I am interested in computer vision and robotics. My current research focuses on:
                <li style="margin: 5px;" >
                  <b>3D scene understanding for robotics</b>
                </li>
                <li style="margin: 5px;" >
                  <b>Lightweight networks and model compression</b>
                </li>
              </p>
              <p style="text-align:center">
                <a href="mailto:xxw21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4G627acAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xuxw98"> Github </a>
              </p>
            </td>
            <td style="padding:0%;width:100%;max-width:100%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/xxw.jpeg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2023-03:</b> One paper on binary sparse convolutional networks is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2022-12:</b> One paper on vision transformer quantization is accepted to <a href="https://ieeexplore.ieee.org/document/9992209">T-PAMI</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DSP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>DSPDet3D: Dynamic Spatial Pruning for 3D Small Object Detection</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Zhihao Sun, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Hongmin Liu, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2305.03716">[arXiv]</a> <a href="https://github.com/xuxw98/DSPDet3D">[Code]</a>
              <br>
              <p> We propose an effective and efficient 3D detector named DSPDet3D for detecting small objects. By scaling up the spatial resolution of feature maps and pruning uninformative scene representaions, DSPDet3D is able to capture detailed local geometric information while keeping low memory footprint and latency.</p>
            </td>
          </tr>

        </tbody></table>
	      
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BSC.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.15493">[arXiv]</a> <a href="https://xuxw98.github.io/files/BSC_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a binary sparse convolutional network called BSC-Net for efficient point cloud analysis. With the presented shifted sparse convolution operation and efficient search method, we reduce the quantization error for sparse convolution without additional computation overhead. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Quantformer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantformer: Learning Extremely Low-precision Vision Transformers</papertitle>
              <br>
              <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Changyuan Wang, <strong>Xiuwei Xu</strong>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 24.31)</em>, 2022
              <br>
              <a href="https://xuxw98.github.io/files/Quantformer.pdf">[PDF]</a> <a href="https://xuxw98.github.io/files/Quantformer-supp.pdf">[Supp]</a> <a href="https://github.com/ZiweiWangTHU/Quantformer">[Code]</a> 
              <br>
              <p> We propose the extremely low-precision vision transformers in 2-4 bits, where a self-attention rank consistency loss and a group-wise quantization strategy are presented for quantization error minimization. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/PointRas.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>PointRas: Uncertainty-Aware Multi-Resolution Learning for Point Cloud Segmentation</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=en">Yu Zheng</a>, <strong>Xiuwei Xu</strong>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE Transactions on Image Processing (<strong>T-IP</strong>, IF: 11.04)</em>, 2022
              <br>
              <a href="https://xuxw98.github.io/files/PointRas.pdf">[PDF]</a>
              <br>
              <p> We propose an uncertainty-aware multi-resolution learning for point cloud segmentation, which can be inserted into any PointNet++-based backbone to iteratively fix the error in predicted semantic mask. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Yifan Wang, <a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=en">Yu Zheng</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.05238">[arXiv]</a> <a href="https://github.com/wyf-ACCEPT/BackToReality">[Code]</a> <a href="https://xuxw98.github.io/files/BR_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with only annotations of object centers. We convert the weak annotations into virtual scenes with synthetic 3D shapes and apply domain adaptation to train a size-aware detector for real scenes. </p>
            </td>
          </tr>

        </tbody></table>

          <!-- <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=NrRf7pUAAAAJ&hl=en"> Yi Wei </a>, <a href="http://b1ueber2y.me/academic.html"> Shaohui Liu</a>, <strong>Yongming Rao</strong>, Wang Zhao, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> <a href="https://weiyithu.github.io/NerfingMVS">[Project page]</a> <a href="https://youtu.be/i-b5lPnYipA">[Video]</a>
              <br>
              <p> We present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr></tbody></table>
      </div> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> Outstanding Graduates (Beijing & Dept. of Automation, Tsinghua University), 2021</li>
                <li style="margin: 5px;"> Innovation Award of Science and Technology, Tsinghua University, 2019-2020</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
		<li style="margin: 5px;"> Teaching Assistant, Pattern recognition and machine learning, 2022 Fall Semester</li>
                <li style="margin: 5px;"> Teaching Assistant, Numerical analysis, 2021 Fall Semester</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> ICCV 2023, ICASSP 2022, VCIP 2022
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> IEEE T-IP
              </li>
            </p>
          </td>
          </tr>
        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
	  <div id="clustrmaps-widget" style="width:5%">
	  <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=yp3s8rdiQW_pbzmBOzWDx2Fv6afIlEpV-k1EZiYIkEY"></script>
	  </div>        
	  <br>
	    &copy; Xiuwei Xu | Last updated: March 5, 2023
</center></p>
</body>

</html>

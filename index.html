<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
  </style>

<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiuwei Xu</title>
  
  <meta name="author" content="Xiuwei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>


<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiuwei Xu | 许修为</name>
              </p>
              <p> 
                I am a third year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2021, I obtained my B.Eng. in the Department of Automation, Tsinghua University.
              </p>
              <p>
              I am interested in computer vision and robotics. My current research focuses on:
                <li style="margin: 5px;" >
                <b style="color:brown">Embodied AI</b> that grounds robotic planning with physical scenes.
              </li>
              <li style="margin: 5px;" >
                <b style="color:green">3D scene perception</b> that accurately and efficiently understands the surrounding 3D scenes captured by robotic agent.
              </li>
               <li style="margin: 5px;" >
                <b style="color:orange">Model compression</b> that reduces the computation and storage cost of neural networks with techniques like quantization and pruning.
              </li>
              </p>
              <p style="text-align:center">
                <a href="mailto:xxw21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4G627acAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xuxw98"> Github </a>
              </p>
            </td>
            <td style="padding:0%;width:100%;max-width:100%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/xxw.jpeg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
        <li style="margin: 5px;" >
                <b>2024-02:</b> Three papers are accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-10:</b> One paper on weakly-supervised point cloud analysis is accepted to <a href="https://ieeexplore.ieee.org/document/">T-PAMI</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-09:</b> One paper of deploying vision transformers on microcontrollers is accepted to <a href="https://nips.cc/">NeurIPS 2023</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2023-03:</b> One paper on binary sparse convolutional networks is accepted to <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Preprint</heading></p>
        <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/EIF-Unknown.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Instruction Following in Unknown Environments</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2406.11818">[arXiv]</a> <a href="https://github.com/Gary3410/TaPA">[Code]</a> <a href="https://gary3410.github.io/eif_unknown/">[Project Page]</a>
              <br>
              <p>Our embodied agent efficiently explores the unknown environment to generate feasible plans with existing objects to accomplish abstract instructions. which can complete complex human instructions such as making breakfast, tidying bedrooms and cleaning bathrooms in house-level scenes.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TAPA.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Task Planning with Large Language Models</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2307.01848">[arXiv]</a> <a href="https://github.com/Gary3410/TaPA">[Code]</a> <a href="https://gary3410.github.io/TaPA/">[Project Page]</a> <a href="https://huggingface.co/spaces/xuxw98/TAPA">[Demo]</a>
              <br>
              <p> We propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DSP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>3D Small Object Detection with Dynamic Spatial Pruning</papertitle>
              <br>
              <strong>Xiuwei Xu*</strong>, Zhihao Sun*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Hongmin Liu, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2305.03716">[arXiv]</a> <a href="https://github.com/xuxw98/DSPDet3D">[Code]</a> <a href="https://xuxw98.github.io/DSPDet3D/">[Project Page]</a>
              <br>
              <p> We propose an effective and efficient 3D detector named DSPDet3D for detecting small objects. By scaling up the spatial resolution of feature maps and pruning uninformative scene representaions, DSPDet3D is able to capture detailed local geometric information while keeping low memory footprint and latency.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <p><heading>Publications</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Online3D.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Memory-based Adapters for Online 3D Scene Perception</papertitle>
              <br>
              <strong>Xiuwei Xu*</strong>, Chong Xia*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.06974">[arXiv]</a> <a href="https://github.com/xuxw98/Online3D">[Code]</a> <a href="https://xuxw98.github.io/Online3D/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/704435537">[中文解读]</a>
              <br>
              <p> We propose a model and task-agnostic plug-and-play module, which converts offline 3D scene perception models (receive reconstructed point clouds) to online perception models (receive streaming RGB-D videos). </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ADPDM.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Towards Accurate Data-free Quantization for Diffusion Models</papertitle>
              <br>
              Changyuan Wang, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://andytang15.github.io/">Yansong Tang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024 (<em><strong style="color:red;">Highlight</strong></em>)
              <br>
              <a href="https://arxiv.org/abs/2305.18723">[arXiv]</a> <a href="https://github.com/ChangyuanWang17/APQ-DM">[Code]</a>
              <br>
              <p> We propose a post-training quantization framework to compress diffusion models, which performs group-wise quantization to minimize rounding errors across time steps and selects generated contents in the optimal time steps for calibration. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR++.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Learning Data-Efficient 3D Object Detector with Shape Guidance</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 23.6)</em>, 2023
              <br>
              <a href="https://xuxw98.github.io/files/BR++.pdf">[PDF]</a> <a href="https://xuxw98.github.io/files/BR++-supp.pdf">[Supp]</a> <a href="https://github.com/xuxw98/BackToReality">[Code]</a> 
              <br>
              <p> We extend BR to BR++ by introducing differentiable label enhancement and label-assisted self-training. Our approach surpasses current weakly-supervised and semi-supervised methods by a large margin, and achieves comparable detection performance with some fully-supervised methods with less than 5% of the labeling labor. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/MCUFormer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory</papertitle>
              <br>
              Yinan Liang, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="https://andytang15.github.io/">Yansong Tang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Thirty-seventh Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2310.16898">[arXiv]</a> <a href="https://github.com/liangyn22/MCUFormer">[Code]</a> <a href="https://mp.weixin.qq.com/s/j4-C2eDSpJPCAPp1XYwEZA">[中文解读]</a>
              <br>
              <p> We propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. </p>
            </td>
          </tr>

        </tbody></table>

        
       <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BSC.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Binarizing Sparse Convolutional Networks for Efficient Point Cloud Analysis</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.15493">[arXiv]</a> <a href="https://xuxw98.github.io/files/BSC_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a binary sparse convolutional network called BSC-Net for efficient point cloud analysis. With the presented shifted sparse convolution operation and efficient search method, we reduce the quantization error for sparse convolution without additional computation overhead. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Quantformer.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Quantformer: Learning Extremely Low-precision Vision Transformers</papertitle>
              <br>
              <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Changyuan Wang, <strong>Xiuwei Xu</strong>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 23.6)</em>, 2022
              <br>
              <a href="https://xuxw98.github.io/files/Quantformer.pdf">[PDF]</a> <a href="https://xuxw98.github.io/files/Quantformer-supp.pdf">[Supp]</a> <a href="https://github.com/ZiweiWangTHU/Quantformer">[Code]</a> 
              <br>
              <p> We propose the extremely low-precision vision transformers in 2-4 bits, where a self-attention rank consistency loss and a group-wise quantization strategy are presented for quantization error minimization. </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Yifan Wang, <a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=en">Yu Zheng</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.05238">[arXiv]</a> <a href="https://github.com/wyf-ACCEPT/BackToReality">[Code]</a> <a href="https://xuxw98.github.io/files/BR_poster.pdf">[Poster]</a>
              <br>
              <p> We propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with only annotations of object centers. We convert the weak annotations into virtual scenes with synthetic 3D shapes and apply domain adaptation to train a size-aware detector for real scenes. </p>
            </td>
          </tr>

        </tbody></table>

          <!-- <div id="click">
            <p align=right><a href="#Show-the-full-publication-list" style="padding:20px;" onclick="showStuff(this);">Full publication list</a></p>
           </div>
           <script>
             function showStuff(txt) {
               document.getElementById("full").style.display = "block";
               document.getElementById("click").style.display = "none";
              //  document.getElementById("selected").style.display = "none";
             }
             </script>

      <div id="full">

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src="images/NerfingMVS.png" alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>NerfingMVS: Guided Optimization of Neural Radiance Fields for Indoor Multi-view Stereo</papertitle>
              <br>
              <a href="https://scholar.google.com/citations?user=NrRf7pUAAAAJ&hl=en"> Yi Wei </a>, <a href="http://b1ueber2y.me/academic.html"> Shaohui Liu</a>, <strong>Yongming Rao</strong>, Wang Zhao, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1"> Jie Zhou </a>
              <br>
              <em>IEEE International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021
              <br>
              <font color="red"><strong>Oral Presentation</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2109.01129">[arXiv]</a> <a href="https://github.com/weiyithu/NerfingMVS">[Code]</a> <a href="https://weiyithu.github.io/NerfingMVS">[Project page]</a> <a href="https://youtu.be/i-b5lPnYipA">[Video]</a>
              <br>
              <p> We present a new multi-view depth estimation method that utilizes both conventional SfM reconstruction and learning-based priors over the recently proposed neural radiance fields (NeRF).</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/VTree.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>V-tree: Efficient KNN Search on Moving Objects with Road-Network Constraints</papertitle>
              <br>
              Bilong Shen, Ying Zhao, Guoliang Li, Weimin Zheng, Yue Qin, Bo Yuan, <strong>Yongming Rao</strong>
              <br>
              <em>IEEE International Conference on Data Engineering (<strong>ICDE</strong>)</em>, 2017
              <br>
              <a href="https://raoyongming.github.io/files/vtree.pdf">[PDF]</a>
              <br>
              <p> We propose a new tree structure for moving objects kNN search with road-network constraints, which can be used in many real-world applications like taxi search.  </p>
            </td>
          </tr></tbody></table>
      </div> -->


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Honors and Awards</heading>
              <p>
                <li style="margin: 5px;"> Outstanding Graduates (Beijing & Dept. of Automation, Tsinghua University), 2021</li>
                <li style="margin: 5px;"> Innovation Award of Science and Technology, Tsinghua University, 2019-2020</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
    <li style="margin: 5px;"> Teaching Assistant, Computer vision, 2024 Spring Semester</li>
                <li style="margin: 5px;"> Teaching Assistant, Pattern recognition and machine learning, 2023 Fall Semester</li>
    <li style="margin: 5px;"> Teaching Assistant, Pattern recognition and machine learning, 2022 Fall Semester</li>
                <li style="margin: 5px;"> Teaching Assistant, Numerical analysis, 2021 Fall Semester</li>
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer / PC Member:</b> NeurIPS 2024, ECCV 2024, ICCV 2023, ICASSP 2022/2023, VCIP 2022
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> IEEE T-IP, T-ITS
              </li>
            </p>
          </td>
          </tr>
        </tbody></table>
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
    <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=26-EpITiANieKUbBkP3kYqH25_ayJEKe2zYZjXiBHyE"></script>
    </div>        
    <br>
      &copy; Xiuwei Xu | Last updated: Feb 27, 2024
</center></p>
</body>

</html>

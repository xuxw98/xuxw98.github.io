<!DOCTYPE HTML>

<style>
  #full {
    display: none;
  }
</style>

<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Xiuwei Xu</title>
  
  <meta name="author" content="Xiuwei Xu">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
  <style>
    .navbar ul {
        list-style-type: none;
        margin: 0;
        padding: 0;
        overflow: hidden;
        border: 1px solid #e7e7e7;
        background-color: #f3f3f3;
    }

    .navbar li {
        float: left;
    }

    .navbar li a {
        display: block;
        color: #666;
        text-align: center;
        padding: 14px 16px;
        text-decoration: none;
    }

    .navbar li a:hover:not(.active) {
        background-color: #ddd;
    }

    .navbar li a.active {
        color: white;
        background-color: #4CAF50;
    }
  </style>
</head>

<div class="navbar">
    <ul>
      <li><a href="index.html">Main Page</a></li>
      <li><a href="pubs.html">Publications</a></li>
      <li><a href="projects.html">Projects</a></li>
      <li style="float:right"><a class="active" href="#about">About</a></li>
    </ul>
</div>

<body>
  <table style="width:100%;max-width:860px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Xiuwei Xu | 许修为</name>
              </p>
              <p> 
                I am a fourth year Ph.D student in the Department of Automation at Tsinghua University, advised by Prof. <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/"> Jiwen Lu </a>. In 2021, I obtained my B.Eng. in the Department of Automation, Tsinghua University.
              </p>
              <p>
              I work on computer vision and robotics. My current research focuses on:
                <li style="margin: 5px;" >
                <b style="color:brown">Embodied AI</b> that grounds robotic planning with physical scenes, especially robotic navigation and mobile manipulation.
              </li>
              <li style="margin: 5px;" >
                <b style="color:green">3D scene perception</b> that accurately and efficiently understands the dynamic 3D scenes captured by robotic agent.
              </li>
               <li style="margin: 5px;" >
                <b style="color:orange">3D reconstruction</b> that builds 3D scenes from raw sensor inputs in online and real-time, especially SLAM and 3D gaussians.
              </li>
              </p>
              <p style="text-align:center">
                <a href="mailto:xxw21@mails.tsinghua.edu.cn">Email</a> &nbsp/&nbsp
                <a href="">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=4G627acAAAAJ&hl=en"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/xuxw98"> Github </a>
              </p>
            </td>
            <td style="padding:0%;width:100%;max-width:100%">
              <img style="width:100%;max-width:100%" alt="profile photo" src="images/xxw.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2025-01:</b> EmbodiedSAM is accepted to <a href="https://www.iclr.cc/Conferences/2025">ICLR 2025</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-09:</b> Two papers on VLM quantization and Zero-shot ObjectNav are accepted to <a href="https://nips.cc/">NeurIPS 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-07:</b> DSPDet3D is accepted to <a href="https://eccv2024.ecva.net/">ECCV 2024</a>.
              </li>
              <li style="margin: 5px;" >
                <b>2024-02:</b> Three papers are accepted to <a href="https://cvpr.thecvf.com/">CVPR 2024</a>.
              </li>
              <!-- <li style="margin: 5px;" >
                <b>2023-10:</b> One paper on weakly-supervised point cloud analysis is accepted to <a href="https://ieeexplore.ieee.org/document/">T-PAMI</a>.
              </li> -->
            </p>
          </td>
        </tr>
      </tbody></table>





        
<!--         <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>
          <td style="text-indent:20px;width:100%;vertical-align:middle">
          <p>
            *Equal contribution, &nbsp;&nbsp; <sup>†</sup>Project leader.
          </p>
          </td>
        </tbody></table> -->

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p>*Equal contribution, &nbsp;&nbsp; <sup>†</sup>Project leader.</p>
              <p><heading>Selected Preprint</heading></p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/TAPA.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Embodied Task Planning with Large Language Models</papertitle>
              <br>
              <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <strong>Xiuwei Xu</strong>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>, Haibin Yan
              <br>
              <em>arXiv</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2307.01848">[arXiv]</a> <a href="https://github.com/Gary3410/TaPA">[Code]</a> <a href="https://gary3410.github.io/TaPA/">[Project Page]</a> <a href="https://huggingface.co/spaces/xuxw98/TAPA">[Demo]</a>
              <br>
              <p> We propose a TAsk Planing Agent (TaPA) in embodied tasks for grounded planning with physical scene constraint, where the agent generates executable plans according to the existed objects in the scene by aligning LLMs with the visual perception models.</p>
            </td>
          </tr>

        </tbody></table>




  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Publications</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/ESAM.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>EmbodiedSAM: Online Segment Any 3D Thing in Real Time</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Huangxing Chen, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>International Conference on Learning Representations (<strong>ICLR</strong>)</em>, 2025 (<em><strong style="color:red;">Oral</strong></em>)
              <br>
              <a href="https://arxiv.org/abs/2408.11811">[arXiv]</a> <a href="https://github.com/xuxw98/ESAM">[Code]</a> <a href="https://xuxw98.github.io/ESAM/">[Project Page]</a>
              <br>
              <p>We presented ESAM, an efficient framework that leverages vision foundation models for online, real-time, fine-grained, generalized and open-vocabulary 3D instance segmentation.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/SG-Nav.gif' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>SG-Nav: Online 3D Scene Graph Prompting for LLM-based Zero-shot Object Navigation</papertitle>
              <br>
              <a href="https://bagh2178.github.io/">Hang Yin</a>*, <strong>Xiuwei Xu*<sup>†</sup></strong>, <a href="https://gary3410.github.io/">Zhenyu Wu</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>Thirty-eighth Conference on Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2410.08189">[arXiv]</a> <a href="https://github.com/bagh2178/SG-Nav">[Code]</a> <a href="https://bagh2178.github.io/SG-Nav/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/909651478">[中文解读]</a>
              <br>
              <p> We propose a training-free object-goal navigation framework by leveraging LLM and VFMs. We construct an online hierarchical 3D scene graph and prompt LLM to exploit structure information contained in subgraphs for zero-shot decision making.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/DSP.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>3D Small Object Detection with Dynamic Spatial Pruning</papertitle>
              <br>
              <strong>Xiuwei Xu*</strong>, Zhihao Sun*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, Hongmin Liu, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2305.03716">[arXiv]</a> <a href="https://github.com/xuxw98/DSPDet3D">[Code]</a> <a href="https://xuxw98.github.io/DSPDet3D/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/714402773">[中文解读]</a>
              <br>
              <p> We propose an effective and efficient 3D detector named DSPDet3D for detecting small objects. By scaling up the spatial resolution of feature maps and pruning uninformative scene representaions, DSPDet3D is able to capture detailed local geometric information while keeping low memory footprint and latency.</p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/Online3D.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Memory-based Adapters for Online 3D Scene Perception</papertitle>
              <br>
              <strong>Xiuwei Xu*</strong>, Chong Xia*, <a href="https://ziweiwangthu.github.io/">Ziwei Wang</a>, <a href="https://scholar.google.com/citations?user=ypxt5UEAAAAJ&hl=zh-CN&oi=ao">Linqing Zhao</a>, <a href="https://duanyueqi.github.io/">Yueqi Duan</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024
              <br>
              <a href="https://arxiv.org/abs/2403.06974">[arXiv]</a> <a href="https://github.com/xuxw98/Online3D">[Code]</a> <a href="https://xuxw98.github.io/Online3D/">[Project Page]</a> <a href="https://zhuanlan.zhihu.com/p/704435537">[中文解读]</a>
              <br>
              <p> We propose a model and task-agnostic plug-and-play module, which converts offline 3D scene perception models (receive reconstructed point clouds) to online perception models (receive streaming RGB-D videos). </p>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
              <img style="width:100%;max-width:100%" src='images/BR++.png' alt="dise">
            </td>
            <td width="75%" valign="center">
              <papertitle>Back to Reality: Weakly-supervised 3D Object Detection with Shape-guided Label Enhancement</papertitle>
              <br>
              <strong>Xiuwei Xu</strong>, Yifan Wang, <a href="https://scholar.google.com/citations?user=J4ZIfhwAAAAJ&hl=en">Yu Zheng</a>, <a href="https://raoyongming.github.io/">Yongming Rao</a>, <a href="https://scholar.google.com/citations?user=6a79aPwAAAAJ&hl=en&authuser=1">Jie Zhou</a>, <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/">Jiwen Lu</a>
              <br>
              <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022
              <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>T-PAMI</strong>, IF: 23.6)</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2203.05238">[arXiv]</a> <a href="https://github.com/wyf-ACCEPT/BackToReality">[Code]</a> <a href="https://xuxw98.github.io/files/BR_poster.pdf">[Poster]</a> <a href="https://xuxw98.github.io/files/BR++.pdf">[PDF (Journal)]</a> <a href="https://xuxw98.github.io/files/BR++-supp.pdf">[Supp (Journal)]</a>
              <br>
              <p> We propose a weakly-supervised approach for 3D object detection, which makes it possible to train a strong 3D detector with only annotations of object centers. We convert the weak annotations into virtual scenes with synthetic 3D shapes and apply domain adaptation to train a size-aware detector for real scenes.</p>
            </td>
          </tr>

        </tbody></table>

          <div id="click">
            <p align=right><a href="pubs.html">Full publication list</a></p>
           </div>



  
  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-bottom:-20px"><tbody>
            <tr>
            <td style="text-indent:20px;width:100%;vertical-align:middle">
              <p><heading>Selected Projects</heading></p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <!-- 左侧视频及滚动按钮 -->
            <td style="padding: 10px; width: 30%; max-width: 30%; text-align: center; vertical-align: middle; position: relative;">
              <div style="position: relative; width: 305px; height: 290px; overflow: hidden; margin-right: 40px;">
                <!-- 视频容器 -->
                <div id="video-container" style="display: flex; transition: transform 0.3s ease;">
                  <div style="width: 305px; text-align: center;">
                    <video style="width: 305px; height: 250px;" controls>
                      <source src="videos/DSPDet3D.mov" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <div style="display: flex; justify-content: center; align-items: center; margin-top: 5px;">
                      <button onclick="scrollVideos(-1)" style="width: 30px; height: 30px; margin-right: 5px;">◀</button>
                      <p style="margin: 0; font-size: 12px; color: #555;">DSPDet3D</p>
                      <button onclick="scrollVideos(1)" style="width: 30px; height: 30px; margin-left: 5px;">▶</button>
                    </div>
                  </div>
                  <div style="width: 305px; text-align: center;">
                    <video style="width: 305px; height: 250px;" controls>
                      <source src="videos/Online3D.mov" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <div style="display: flex; justify-content: center; align-items: center; margin-top: 5px;">
                      <button onclick="scrollVideos(-1)" style="width: 30px; height: 30px; margin-right: 5px;">◀</button>
                      <p style="margin: 0; font-size: 12px; color: #555;">Online3D</p>
                      <button onclick="scrollVideos(1)" style="width: 30px; height: 30px; margin-left: 5px;">▶</button>
                    </div>
                  </div>
                  <div style="width: 305px; text-align: center;">
                    <video style="width: 305px; height: 250px;" controls>
                      <source src="videos/EmbodiedSAM.mov" type="video/mp4">
                      Your browser does not support the video tag.
                    </video>
                    <div style="display: flex; justify-content: center; align-items: center; margin-top: 5px;">
                      <button onclick="scrollVideos(-1)" style="width: 30px; height: 30px; margin-right: 5px;">◀</button>
                      <p style="margin: 0; font-size: 12px; color: #555;">EmbodiedSAM</p>
                      <button onclick="scrollVideos(1)" style="width: 30px; height: 30px; margin-left: 5px;">▶</button>
                    </div>
                  </div>
                </div>
              </div>
            </td>

            <!-- 右侧文字内容 -->
            <td width="70%" style="vertical-align: middle; padding: 10px; text-align: left;">
              <h3>Efficient and Online 3D Scene Perception</h3>
              <p>In this project, we study how to make 3D scene perception methods applicable for embodied scenarios such as robotic planning and interaction. Although various research have been conducted on 3D scene perception, it is still very challenging to (1) process large-scale 3D scenes with both high fine granularity and fast speed and (2) perceive the 3D scenes in an online and real-time manner that directly consumes streaming RGB-D video as input. We solve these problems in below works:
                <li style="margin: 5px;">
                  <a href="https://xuxw98.github.io/DSPDet3D/">DSPDet3D</a> which is able to detect almost everything (small and large) given a building-level 3D scene, within 2s on a single GPU.
                </li>
                <li style="margin: 5px;">
                  <a href="https://xuxw98.github.io/Online3D/">Online3D</a> which converts offline 3D scene perception models (receive reconstructed point clouds) to online perception models (receive streaming RGB-D videos) in a model and task-agnostic plug-and-play manner.
                </li>
                <li style="margin: 5px;">
                  <a href="https://xuxw98.github.io/ESAM/">EmbodiedSAM</a> which online segments any 3D thing in real time.
                </li>
              </p>
            </td>
          </tr>
        </table>

        <script>
          let currentIndex = 0;

          function scrollVideos(direction) {
            const container = document.getElementById('video-container');
            const videoWidth = 305; // 视频宽度
            const totalVideos = container.children.length;

            // 计算新的索引
            currentIndex = Math.max(0, Math.min(currentIndex + direction, totalVideos - 1));

            // 更新滚动位置
            container.style.transform = `translateX(-${currentIndex * videoWidth}px)`;
          }
        </script>



  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;margin-top:10px"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Grants and Awards</heading>
              <p>
                <li style="margin: 5px;"> NSFC Youth Student Research Project (PhD) / 国家自然科学基金青年学生基础研究项目（博士研究生）, 2025-2026</li>
                <li style="margin: 5px;"> National Scholarship, 2024</li>
                <li style="margin: 5px;"> Outstanding Graduates (Beijing & Dept. of Automation, Tsinghua University), 2021</li>
                <li style="margin: 5px;"> Innovation Award of Science and Technology, Tsinghua University, 2019-2020</li>
              </p>
            </td>
          </tr>
        </tbody></table>



  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Teaching</heading>
              <p>
    <li style="margin: 5px;"> Teaching Assistant, Computer vision, 2024 Spring Semester</li>
                <li style="margin: 5px;"> Teaching Assistant, Pattern recognition and machine learning, 2023 Fall Semester</li>
    <li style="margin: 5px;"> Teaching Assistant, Pattern recognition and machine learning, 2022 Fall Semester</li>
                <li style="margin: 5px;"> Teaching Assistant, Numerical analysis, 2021 Fall Semester</li>
              </p>
            </td>
          </tr>
        </tbody></table>



  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Academic Services</heading>
            <p>
              <li style="margin: 5px;"> 
                <b>Conference Reviewer:</b> CVPR 2025, ICLR 2025, NeurIPS 2024, ECCV 2024, ICCV 2023, ICASSP 2022/2023, VCIP 2022
              </li>
              <li style="margin: 5px;"> 
                <b>Journal Reviewer:</b> IJCV, T-IP, T-ITS, T-MM, T-CSVT
              </li>
            </p>
          </td>
          </tr>
        </tbody></table>



  
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
 
<p><center>
    <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=26-EpITiANieKUbBkP3kYqH25_ayJEKe2zYZjXiBHyE"></script>
    </div>        
    <br>
      &copy; Xiuwei Xu | Last updated: Nov 13, 2024
</center></p>
</body>

</html>
